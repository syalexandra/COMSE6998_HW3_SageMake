{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SMS Spam Classifier</h1>\n",
    "<br />\n",
    "This notebook shows how to implement a basic spam classifier for SMS messages using Apache MXNet as deep learning framework.\n",
    "The idea is to use the SMS spam collection dataset available at <a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\">https://archive.ics.uci.edu/ml/datasets/sms+spam+collection</a> to train and deploy a neural network model by leveraging on the built-in open-source container for Apache MXNet available in Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by setting some configuration variables and getting the Amazon SageMaker session and the current execution role, using the Amazon SageMaker high-level SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::233542734977:role/service-role/AmazonSageMaker-ExecutionRole-20220406T232607\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "bucket_name = 'spam-detection-hw3-bucket'\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_key_prefix = 'sms-spam-classifier'\n",
    "vocabulary_length = 9013\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now download the spam collection dataset, unzip it and read the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  198k  100  198k    0     0   303k      0 --:--:-- --:--:-- --:--:--  303k\n",
      "Archive:  dataset/smsspamcollection.zip\n",
      "  inflating: dataset/SMSSpamCollection  \n",
      "  inflating: dataset/readme          \n",
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tOk lar... Joking wif u oni...\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "ham\tU dun say so early hor... U c already then say...\n",
      "ham\tNah I don't think he goes to usf, he lives around here though\n",
      "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p dataset\n",
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip -o dataset/smsspamcollection.zip\n",
    "!unzip -o dataset/smsspamcollection.zip -d dataset\n",
    "!head -10 dataset/SMSSpamCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the dataset into a Pandas dataframe and execute some data preparation.\n",
    "More specifically we have to:\n",
    "<ul>\n",
    "    <li>replace the target column values (ham/spam) with numeric values (0/1)</li>\n",
    "    <li>tokenize the sms messages and encode based on word counts</li>\n",
    "    <li>split into train and test sets</li>\n",
    "    <li>upload to a S3 bucket for training</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sms_spam_classifier_utilities import one_hot_encode\n",
    "from sms_spam_classifier_utilities import vectorize_sequences\n",
    "\n",
    "df = pd.read_csv('dataset/SMSSpamCollection', sep='\\t', header=None)\n",
    "df[df.columns[0]] = df[df.columns[0]].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "targets = df[df.columns[0]].values\n",
    "messages = df[df.columns[1]].values\n",
    "\n",
    "# one hot encoding for each SMS message\n",
    "one_hot_data = one_hot_encode(messages, vocabulary_length)\n",
    "encoded_messages = vectorize_sequences(one_hot_data, vocabulary_length)\n",
    "\n",
    "df2 = pd.DataFrame(encoded_messages)\n",
    "df2.insert(0, 'spam', targets)\n",
    "\n",
    "# Split into training and validation sets (80%/20% split)\n",
    "split_index = int(np.ceil(df.shape[0] * 0.8))\n",
    "train_set = df2[:split_index]\n",
    "val_set = df2[split_index:]\n",
    "\n",
    "train_set.to_csv('dataset/sms_train_set.gz', header=False, index=False, compression='gzip')\n",
    "val_set.to_csv('dataset/sms_val_set.gz', header=False, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>9003</th>\n",
       "      <th>9004</th>\n",
       "      <th>9005</th>\n",
       "      <th>9006</th>\n",
       "      <th>9007</th>\n",
       "      <th>9008</th>\n",
       "      <th>9009</th>\n",
       "      <th>9010</th>\n",
       "      <th>9011</th>\n",
       "      <th>9012</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4460</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4462</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1114 rows × 9014 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam    0    1    2    3    4    5    6    7    8  ...  9003  9004  \\\n",
       "4458     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4459     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4460     1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4461     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4462     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "5567     1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "5568     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "5569     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "5570     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "5571     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "      9005  9006  9007  9008  9009  9010  9011  9012  \n",
       "4458   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4459   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4460   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4461   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4462   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "5567   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5568   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5569   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5570   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5571   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[1114 rows x 9014 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to upload the two files back to Amazon S3 in order to be accessed by the Amazon SageMaker training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "target_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "with open('dataset/sms_train_set.gz', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/train/sms_train_set.gz'.format(bucket_key_prefix))\n",
    "    \n",
    "with open('dataset/sms_val_set.gz', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/val/sms_val_set.gz'.format(bucket_key_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training the model with MXNet</h2>\n",
    "\n",
    "We are now ready to run the training using the Amazon SageMaker MXNet built-in container. First let's have a look at the script defining our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "\r\n",
      "import pip\r\n",
      "\r\n",
      "try:\r\n",
      "    from pip import main as pipmain\r\n",
      "except:\r\n",
      "    from pip._internal import main as pipmain\r\n",
      "\r\n",
      "pipmain(['install', 'pandas'])\r\n",
      "import pandas\r\n",
      "\r\n",
      "#logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,\r\n",
      "          num_gpus, num_cpus, hosts, current_host, **kwargs):\r\n",
      "    # SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\r\n",
      "    # the current container environment, but here we just use simple cpu context.\r\n",
      "    ctx = mx.cpu()\r\n",
      "\r\n",
      "    # retrieve the hyperparameters and apply some defaults in case they are not provided.\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.01)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "\r\n",
      "    train_data_path = channel_input_dirs['train']\r\n",
      "    val_data_path = channel_input_dirs['val']\r\n",
      "    train_data = get_train_data(train_data_path, batch_size)\r\n",
      "    val_data = get_val_data(val_data_path, batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = define_network()\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Normal(sigma=1.), ctx=ctx)\r\n",
      "    \r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\r\n",
      "                            {'learning_rate': learning_rate, 'momentum': momentum},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    \r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        \r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        for i, (data, label) in enumerate(train_data):\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = data.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "            \r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "            L.backward()\r\n",
      "\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "\r\n",
      "            # update metric at last.\r\n",
      "            sigmoid_output = output.sigmoid() \r\n",
      "            prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "            metric.update([label], [prediction])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_data)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "def define_network():\r\n",
      "    net = nn.Sequential()\r\n",
      "    with net.name_scope():\r\n",
      "        net.add(nn.Dense(64, activation=\"relu\"))\r\n",
      "        net.add(nn.Dense(1))\r\n",
      "    return net\r\n",
      "\r\n",
      "def get_train_data(data_path, batch_size):\r\n",
      "    print('Train data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_train_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=True)\r\n",
      "\r\n",
      "def get_val_data(data_path, batch_size):\r\n",
      "    print('Validation data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_val_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=False)\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for data, label in val_data:\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        \r\n",
      "        output = net(data)\r\n",
      "        sigmoid_output = output.sigmoid() \r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        metric.update([label], [prediction])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    net = gluon.nn.SymbolBlock(\r\n",
      "        outputs=mx.sym.load('%s/model.json' % model_dir),\r\n",
      "        inputs=mx.sym.var('data'))\r\n",
      "    \r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    try:\r\n",
      "        parsed = json.loads(data)\r\n",
      "        nda = mx.nd.array(parsed)\r\n",
      "        \r\n",
      "        output = net(nda)\r\n",
      "        sigmoid_output = output.sigmoid()\r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        output_obj = {}\r\n",
      "        output_obj['predicted_label'] = prediction.asnumpy().tolist()\r\n",
      "        output_obj['predicted_probability'] = sigmoid_output.asnumpy().tolist()\r\n",
      "\r\n",
      "        response_body = json.dumps(output_obj)\r\n",
      "        return response_body, output_content_type\r\n",
      "    except Exception as ex:\r\n",
      "        response_body = '{error: }' + str(ex)\r\n",
      "        return response_body, output_content_type\r\n",
      "    "
     ]
    }
   ],
   "source": [
    "!cat 'sms_spam_classifier_mxnet_script.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the training using the MXNet estimator object of the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-07 00:13:23 Starting - Starting the training job...\n",
      "2022-04-07 00:13:46 Starting - Preparing the instances for trainingProfilerReport-1649290402: InProgress\n",
      "......\n",
      "2022-04-07 00:14:53 Downloading - Downloading input data.........\n",
      "2022-04-07 00:16:14 Training - Training image download completed. Training in progress..\u001b[34m2022-04-07 00:16:16,586 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2022-04-07 00:16:16,588 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-07 00:16:16,598 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch_size\":100,\"epochs\":20,\"learning_rate\":0.01}', 'SM_USER_ENTRY_POINT': 'sms_spam_classifier_mxnet_script.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"train\",\"val\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'sms_spam_classifier_mxnet_script', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '8', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://spam-detection-hw3-bucket/sms-spam-classifier/code/sms-spam-classifier-mxnet-2022-04-07-00-13-22-691/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":100,\"epochs\":20,\"learning_rate\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sms-spam-classifier-mxnet-2022-04-07-00-13-22-691\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://spam-detection-hw3-bucket/sms-spam-classifier/code/sms-spam-classifier-mxnet-2022-04-07-00-13-22-691/source/sourcedir.tar.gz\",\"module_name\":\"sms_spam_classifier_mxnet_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sms_spam_classifier_mxnet_script.py\"}', 'SM_USER_ARGS': '[\"--batch_size\",\"100\",\"--epochs\",\"20\",\"--learning_rate\",\"0.01\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train', 'SM_CHANNEL_VAL': '/opt/ml/input/data/val', 'SM_HP_BATCH_SIZE': '100', 'SM_HP_EPOCHS': '20', 'SM_HP_LEARNING_RATE': '0.01'}\u001b[0m\n",
      "\u001b[34m2022-04-07 00:16:16,995 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-04-07 00:16:16,995 sagemaker-training-toolkit ERROR    framework error: \u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/trainer.py\", line 85, in train\n",
      "    entrypoint()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_mxnet_container/training.py\", line 93, in main\n",
      "    train(environment.Environment())\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_mxnet_container/training.py\", line 89, in train\n",
      "    runner_type=runner_type)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/entry_point.py\", line 92, in run\n",
      "    files.download_and_extract(uri=uri, path=environment.code_dir)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/files.py\", line 131, in download_and_extract\n",
      "    s3_download(uri, dst)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/files.py\", line 167, in s3_download\n",
      "    s3.Bucket(bucket).download_file(key, dst)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/boto3/s3/inject.py\", line 246, in bucket_download_file\n",
      "    ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/boto3/s3/inject.py\", line 172, in download_file\n",
      "    extra_args=ExtraArgs, callback=Callback)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/boto3/s3/transfer.py\", line 307, in download_file\n",
      "    future.result()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/s3transfer/futures.py\", line 106, in result\n",
      "    return self._coordinator.result()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/s3transfer/futures.py\", line 265, in result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.6/site-packages/s3transfer/tasks.py\", line 255, in _main\n",
      "    self._submit(transfer_future=transfer_future, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/s3transfer/download.py\", line 343, in _submit\n",
      "    **transfer_future.meta.call_args.extra_args\n",
      "  File \"/usr/local/lib/python3.6/site-packages/botocore/client.py\", line 357, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/botocore/client.py\", line 676, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\u001b[0m\n",
      "\u001b[34mbotocore.exceptions.ClientError: An error occurred (404) when calling the HeadObject operation: Not Found\u001b[0m\n",
      "\u001b[34mAn error occurred (404) when calling the HeadObject operation: Not Found\u001b[0m\n",
      "\n",
      "2022-04-07 00:16:49 Uploading - Uploading generated training model\n",
      "2022-04-07 00:16:49 Failed - Training job failed\n",
      "ProfilerReport-1649290402: Stopping\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job sms-spam-classifier-mxnet-2022-04-07-00-13-22-691: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/trainer.py\", line 85, in train\n    entrypoint()\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_mxnet_container/training.py\", line 93, in main\n    train(environment.Environment())\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_mxnet_container/training.py\", line 89, in train\n    runner_type=runner_type)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/entry_point.py\", line 92, in run\n    files.download_and_extract(uri=uri, path=environment.code_dir)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/files.py\", line 131, in download_and_extract\n    s3_download(uri, dst)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/files.py\", line 167, in s3_download\n    s3.Bucket(bucket).download_file(key, dst)\n  File \"/usr/local/lib/python3.6/site-packages/boto3/s3/inject.py\", line 246, in bucket_download_file\n    ExtraArgs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ee58b36b85e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m  'val': 's3://{0}/{1}/val/'.format(bucket_name, bucket_key_prefix)}\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3753\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3754\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3755\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3304\u001b[0m                 ),\n\u001b[1;32m   3305\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3306\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3307\u001b[0m             )\n\u001b[1;32m   3308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job sms-spam-classifier-mxnet-2022-04-07-00-13-22-691: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/trainer.py\", line 85, in train\n    entrypoint()\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_mxnet_container/training.py\", line 93, in main\n    train(environment.Environment())\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_mxnet_container/training.py\", line 89, in train\n    runner_type=runner_type)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/entry_point.py\", line 92, in run\n    files.download_and_extract(uri=uri, path=environment.code_dir)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/files.py\", line 131, in download_and_extract\n    s3_download(uri, dst)\n  File \"/usr/local/lib/python3.6/site-packages/sagemaker_training/files.py\", line 167, in s3_download\n    s3.Bucket(bucket).download_file(key, dst)\n  File \"/usr/local/lib/python3.6/site-packages/boto3/s3/inject.py\", line 246, in bucket_download_file\n    ExtraArgs"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "output_path = 's3://{0}/{1}/output'.format(bucket_name, bucket_key_prefix)\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, bucket_key_prefix)\n",
    "\n",
    "m = MXNet('sms_spam_classifier_mxnet_script.py',\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          train_instance_type='ml.c5.2xlarge',\n",
    "          output_path=output_path,\n",
    "          base_job_name='sms-spam-classifier-mxnet',\n",
    "          framework_version=\"1.6.0\",\n",
    "          py_version=\"py3\",\n",
    "          code_location = code_location,\n",
    "          hyperparameters={'batch_size': 100,\n",
    "                         'epochs': 20,\n",
    "                         'learning_rate': 0.01})\n",
    "\n",
    "inputs = {'train': 's3://{0}/{1}/train/'.format(bucket_name, bucket_key_prefix),\n",
    " 'val': 's3://{0}/{1}/val/'.format(bucket_name, bucket_key_prefix)}\n",
    "\n",
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:red\">THE FOLLOWING STEPS ARE NOT MANDATORY IF YOU PLAN TO DEPLOY TO AWS LAMBDA AND ARE INCLUDED IN THIS NOTEBOOK FOR EDUCATIONAL PURPOSES.</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deploying the model</h2>\n",
    "\n",
    "Let's deploy the trained model to a real-time inference endpoint fully-managed by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_pred = m.deploy(initial_instance_count=1,\n",
    "                      instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Executing Inferences</h2>\n",
    "\n",
    "Now, we can invoke the Amazon SageMaker real-time endpoint to execute some inferences, by providing SMS messages and getting the predicted label (SPAM = 1, HAM = 0) and the related probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.model import MXNetPredictor\n",
    "from sms_spam_classifier_utilities import one_hot_encode\n",
    "from sms_spam_classifier_utilities import vectorize_sequences\n",
    "\n",
    "# Uncomment the following line to connect to an existing endpoint.\n",
    "# mxnet_pred = MXNetPredictor('<endpoint_name>')\n",
    "\n",
    "test_messages = [\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\"]\n",
    "one_hot_test_messages = one_hot_encode(test_messages, vocabulary_length)\n",
    "encoded_test_messages = vectorize_sequences(one_hot_test_messages, vocabulary_length)\n",
    "\n",
    "result = mxnet_pred.predict(encoded_test_messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning-up</h2>\n",
    "\n",
    "When done, we can delete the Amazon SageMaker real-time inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_pred.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
